{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA Preparations for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import *\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import math\n",
    "import glob\n",
    "from random import *\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/c_thititanapak/latent_image_processed_gae_96/'\n",
    "\n",
    "high_blur_list_train = [f for f in glob.glob(PATH + 'high_blur_*.BMP')]\n",
    "high_blur_list_train.sort()\n",
    "\n",
    "original_list_train = [f for f in glob.glob(PATH + 'original_*.BMP')]\n",
    "original_list_train.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/c_thititanapak/latent_image_processed_gae_96_test/'\n",
    "\n",
    "high_blur_list_test = [f for f in glob.glob(PATH + 'high_blur_*.BMP')]\n",
    "high_blur_list_test.sort()\n",
    "\n",
    "original_list_test = [f for f in glob.glob(PATH + 'original_*.BMP')]\n",
    "original_list_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_list(image_list):\n",
    "    result_image_list = []\n",
    "\n",
    "    if (len(image_list) != 0):\n",
    "        image_list.sort()\n",
    "        for i in range(len(image_list)):\n",
    "            temp = cv2.imread(image_list[i], 0)/255.0\n",
    "#             temp.reshape(96,96,1)\n",
    "            result_image_list.append(temp)\n",
    "        \n",
    "    return result_image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = make_image_list(high_blur_list_train)\n",
    "train_y = make_image_list(original_list_train)\n",
    "\n",
    "test_x = make_image_list(high_blur_list_test)\n",
    "test_y = make_image_list(original_list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/home/c_thititanapak/latent_image_processed_gae_96/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_list = [f for f in glob.glob(PATH + \"*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_path_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE gae model\n",
    "\n",
    "<b><i>input</i></b><br>\n",
    "<u>Problem latent image of size 96*96</u>\n",
    "\n",
    "<b><i>output</i></b><br>\n",
    "<u>Enhanced latent image of size 96*96</u>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE AUTO ENCODER Assume cov2d resize as 0.5 \n",
    "# def define_autoencoder(image_shape=(96,96,1)):\n",
    "\n",
    "#     iterations = 0\n",
    "#     temp = image_shape[0]\n",
    "\n",
    "#     while(temp %2 == 0):\n",
    "#         temp = temp/2\n",
    "#         iterations = iterations+1\n",
    "#         print(temp)\n",
    "        \n",
    "#     print(iterations)\n",
    "#     input_layer = Input(shape=image_shape)\n",
    "#     x = input_layer\n",
    "#     for i in range(iterations):\n",
    "#         if (i != 0):\n",
    "#             if (i == iterations-1 and iterations % 2 == 0):\n",
    "#                 x = Conv2D(N_CONV_FILTER, kernel_size=(3,3), padding='same')(x)\n",
    "#             else:\n",
    "#                 x = Conv2D(N_CONV_FILTER/2**(iterations-i), kernel_size=(3,3), strides=(2,2), padding='same')(x)\n",
    "#         else:\n",
    "#             x = Conv2D(N_CONV_FILTER/2**(iterations-i), kernel_size=(3,3), strides=(2,2), padding='same')(input_layer)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "#     if (iterations % 2 != 0):\n",
    "#         x = Conv2D(N_CONV_FILTER, kernel_size=(3,3), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = LeakyReLU(alpha=0.2)(x)\n",
    "        \n",
    "#     for i in range(iterations):\n",
    "#         x = Conv2DTranspose(N_CONV_FILTER/2**i, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "#         x = ReLU()(x)\n",
    "        \n",
    "#     output_layer = Conv2DTranspose(1, kernel_size=(image_shape[0],image_shape[1]), padding='same')(x)\n",
    "#     model = Model(input_layer,output_layer)\n",
    "    \n",
    "#     return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE AUTO ENCODER Assume cov2d resize as 0.5 \n",
    "# min_ks = minimum kernel size eg. (3,3)\n",
    "def define_autoencoder(image_shape=(96,96,1), min_n_filter=32, min_ks=3):\n",
    "    if image_shape[0] != image_shape[1]:\n",
    "        print('image shape should be square')\n",
    "        return\n",
    "    \n",
    "    iterations = 0\n",
    "    temp = image_shape[0]\n",
    "    \n",
    "    while(temp % 2 == 0):\n",
    "        temp = temp / 2\n",
    "        iterations = iterations + 1\n",
    "    max_ks = min_ks+(2*iterations)\n",
    "    # max_ks = 3+(2*5) = 13\n",
    "    ## ENCODER ##\n",
    "    input_layer = Input(shape=image_shape)\n",
    "    x = input_layer\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        asc_n_filter = min_n_filter * (2 ** i)\n",
    "        desc_kernel_size = (max_ks - (2 * i), max_ks - (2 * i))\n",
    "        \n",
    "        if (i != 0):\n",
    "            x = Conv2D(asc_n_filter, kernel_size=desc_kernel_size, strides=(2,2), padding='same')(x)\n",
    "        else:\n",
    "            x = Conv2D(min_n_filter, kernel_size=(max_ks,max_ks), strides=(2,2), padding='same')(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    ## DECODER ##\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        desc_n_filter = min_n_filter * (2 ** (iterations - i - 2))\n",
    "        asc_kernel_size = (min_ks + (2 * (i + 1)),min_ks + (2 * (i + 1)))\n",
    "        \n",
    "        x = Conv2DTranspose(desc_n_filter, kernel_size=asc_kernel_size, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = ReLU()(x)\n",
    "   \n",
    "    \n",
    "    output_layer = Conv2DTranspose(1, kernel_size=(image_shape[0],image_shape[1]), padding='same')(x)\n",
    "    model = Model(input_layer,output_layer)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 96, 96, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 48, 48, 32)        5440      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 24, 24, 64)        247872    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 128)       663680    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         1605888   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 6, 6, 256)         3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 12, 12, 128)       1605760   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 24, 24, 64)        663616    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 48, 48, 32)        247840    \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DT (None, 96, 96, 16)        86544     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 96, 96, 16)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 96, 96, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DT (None, 96, 96, 1)         147457    \n",
      "=================================================================\n",
      "Total params: 11,834,417\n",
      "Trainable params: 11,831,441\n",
      "Non-trainable params: 2,976\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_padding(image, padding_size):\n",
    "    if padding_size <= 0:\n",
    "        return image\n",
    "\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "\n",
    "    zero_padding_image = np.zeros([height + (padding_size * 2), width + (padding_size * 2)], dtype=np.uint8)\n",
    "    zero_padding_height = zero_padding_image.shape[0]\n",
    "    zero_padding_width = zero_padding_image.shape[1]\n",
    "\n",
    "    row_start = padding_size\n",
    "    col_start = padding_size\n",
    "    row_end = zero_padding_height - padding_size\n",
    "    col_end = zero_padding_width - padding_size\n",
    "\n",
    "    zero_padding_image[row_start:row_end, col_start:col_end] = image\n",
    "\n",
    "    return zero_padding_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convolute(image, filter, mode='sum'):\n",
    "#     height, width = image.shape\n",
    "#     result_image = np.zeros(image.shape, dtype=np.uint8)\n",
    "#     padding_size = filter.shape[0] // 2\n",
    "#     zero_padding_image = zero_padding(image, padding_size)\n",
    "#     plt.imshow(zero_padding_image), plt.show()\n",
    "#     flipped_filter = np.flip(filter)\n",
    "\n",
    "#     for i in range(padding_size, height):\n",
    "#         for j in range(padding_size, width):\n",
    "#             conv_matrix = zero_padding_image[i-padding_size:i+padding_size+1, j-padding_size:j+padding_size+1] * flipped_filter\n",
    "#         if mode == 'sum':\n",
    "#             result_image[i][j] = np.absolute((np.sum(conv_matrix)).astype(np.uint8))\n",
    "#         elif mode == 'median':\n",
    "#             result_image[i][j] = np.absolute((np.median(conv_matrix)).astype(np.uint8))\n",
    "#         elif mode == 'average':\n",
    "#             result_image[i][j] = np.absolute((np.mean(conv_matrix)).astype(np.uint8))\n",
    "    \n",
    "#     return result_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_gradient(images):\n",
    "#     image = tf.Variable(image)\n",
    "    \n",
    "    sobel_0 = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "    sobel_45 = np.array([[-1, -1, 0], [-1, 0, 1], [0, 1, 1]])\n",
    "    sobel_90 = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
    "    sobel_135 = np.array([[0, -1, -1], [1, 0, -1], [1, 1, 0]])\n",
    "    \n",
    "    grads_0 = []\n",
    "    grads_45 = []\n",
    "    grads_90 = []\n",
    "    grads_135 = []\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        grad_0 = np.zeros(image.shape, dtype=np.int16)\n",
    "        grad_45 = np.zeros(image.shape, dtype=np.int16)\n",
    "        grad_90 = np.zeros(image.shape, dtype=np.int16)\n",
    "        grad_135 = np.zeros(image.shape, dtype=np.int16)\n",
    "\n",
    "    #     img = image.numpy()\n",
    "    #     print(img)\n",
    "#         print(image.shape)\n",
    "\n",
    "        grad_0 = cv2.filter2D(image.reshape(96,96), -1, sobel_0)\n",
    "        grad_45 = cv2.filter2D(image.reshape(96,96), -1, sobel_45)\n",
    "        grad_90 = cv2.filter2D(image.reshape(96,96), -1, sobel_90)\n",
    "        grad_135 = cv2.filter2D(image.reshape(96,96), -1, sobel_135)\n",
    "        \n",
    "        grads_0.append(grad_0)\n",
    "        grads_45.append(grad_45)\n",
    "        grads_90.append(grad_90)\n",
    "        grads_135.append(grad_135)\n",
    "\n",
    "    return grads_0, grads_45, grads_90, grads_135\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/3149279/optimal-sigma-for-gaussian-filtering-of-an-image\n",
    "#http://www.cse.dmu.ac.uk/~sexton/WWWPages/HIPR/html/gsmooth.html\n",
    "\n",
    "#https://stackoverflow.com/questions/61394826/how-do-i-get-to-show-gaussian-kernel-for-2d-opencv\n",
    "def gaussian_kernel_smoothing(kernel_len = 21, sigma=3):\n",
    "    kernel = np.zeros((kernel_len, kernel_len))\n",
    "    \n",
    "    # a dirac delta \n",
    "    kernel[kernel_len//2, kernel_len//2] = 1\n",
    "    \n",
    "    # a gaussian blur of dirac delta is the 2D gaussian mask filter\n",
    "    return cv2.GaussianBlur(kernel, (kernel_len,kernel_len), sigmaX=sigma, sigmaY=sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_orientation(images):\n",
    "#     image = tf.Variable(image)\n",
    "    gauss_block = np.array([gaussian_kernel_smoothing(7, 1)], dtype= np.float32)\n",
    "    gauss_orient = np.array([gaussian_kernel_smoothing(31, 5)], dtype= np.float32)\n",
    "    \n",
    "    sobel_0 = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "    sobel_90 = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "    \n",
    "    # Get gaussian gradient\n",
    "#     gauss_gradient_x = cv2.filter2D(gauss_block, -1, sobel_0)\n",
    "#     gauss_gradient_y = cv2.filter2D(gauss_block, -1, sobel_90) # >>> 0 ???\n",
    "#     gauss_gradient_y = cv2.filter2D(np.transpose(gauss_block), -1, sobel_0)\n",
    "    gauss_gradient_x = cv2.filter2D(gauss_block.reshape(7,7), -1, sobel_0)\n",
    "    gauss_gradient_y = cv2.filter2D(gauss_block.reshape(7,7), -1, sobel_90)\n",
    "    \n",
    "#     print(\"G Y\")\n",
    "#     print(gauss_gradient_y, gauss_gradient_x)\n",
    "    \n",
    "    # Get image gradients\n",
    "#     print(image.shape)\n",
    "    \n",
    "    orient_imgs = []\n",
    "    reliabilities = []\n",
    "    for i, image in enumerate(images):\n",
    "        \n",
    "        gauss_img_x = cv2.filter2D(image.reshape(96,96), -1, gauss_gradient_x.reshape(7,7))\n",
    "        gauss_img_y = cv2.filter2D(image.reshape(96,96), -1, gauss_gradient_y.reshape(7,7))\n",
    "\n",
    "    #     plt.imshow(gauss_img_x,'gray'), plt.show()\n",
    "    #     plt.imshow(gauss_img_y,'gray'),plt.show()\n",
    "\n",
    "        gauss_img_xx = gauss_img_x ** 2\n",
    "        gauss_img_xy = gauss_img_x * gauss_img_y\n",
    "        gauss_img_yy = gauss_img_y ** 2\n",
    "\n",
    "    #     print(gauss_img_xy[45])\n",
    "    #     plt.imshow(gauss_img_xy,'gray'), plt.show()\n",
    "\n",
    "        gauss_img_xx = cv2.filter2D(gauss_img_xx, -1, gauss_orient.reshape(31,31))\n",
    "        gauss_img_xy = 2 * cv2.filter2D(gauss_img_xy, -1, gauss_orient.reshape(31,31))\n",
    "        gauss_img_yy = cv2.filter2D(gauss_img_yy, -1, gauss_orient.reshape(31,31))\n",
    "    #     print(gauss_img_xy)\n",
    "    #     plt.imshow(gauss_img_xy,'gray'), plt.show()\n",
    "\n",
    "        # get orient img\n",
    "        denom = np.sqrt(gauss_img_xy**2 + (gauss_img_xx - gauss_img_yy)**2 + 1**(-12))\n",
    "\n",
    "        sin2theta = gauss_img_xy / denom\n",
    "        cos2theta = (gauss_img_xx - gauss_img_yy) / denom\n",
    "\n",
    "        # smooth double angle of orient img\n",
    "        sin2theta = cv2.filter2D(sin2theta, -1, gauss_orient.reshape(31,31))\n",
    "        cos2theta = cv2.filter2D(cos2theta, -1, gauss_orient.reshape(31,31))\n",
    "    #     print(sin2theta, cos2theta)\n",
    "\n",
    "    #     print('log')\n",
    "    #     print(np.pi/2.0 + np.arctan2(sin2theta, cos2theta) / 2.0)\n",
    "\n",
    "        orient_img = np.pi/2.0 + np.arctan2(sin2theta, cos2theta) / 2.0\n",
    "    #     plt.imshow(orient_img), plt.show()\n",
    "        # Imin Imax Rscore and coherence\n",
    "        Imin = (gauss_img_yy + gauss_img_xx) / 2.0 - (gauss_img_xx - gauss_img_yy) * cos2theta / 2.0 - gauss_img_xy * sin2theta / 2.0 \n",
    "        Imax = gauss_img_yy + gauss_img_xx - Imin\n",
    "\n",
    "        reliability = 1 - Imin / (Imax + 0.001)\n",
    "    #     print('denom HERE!')\n",
    "    #     print(denom.shape)\n",
    "        if (denom.all() <= 0.001):\n",
    "            reliability = 0\n",
    "\n",
    "        coherence = ((Imax - Imin) / (Imax + Imin + 1 ** (-12))) ** 2\n",
    "        \n",
    "        orient_imgs.append(orient_img)\n",
    "        reliabilities.append(reliability)\n",
    "    \n",
    "    return orient_imgs, reliabilities, coherence\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(a,b):\n",
    "    return tf.reduce_mean((a -b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/custom-loss-function-in-tensorflow-2-0-d8fa35405e4e\n",
    "#https://stackoverflow.com/questions/34875944/how-to-write-a-custom-loss-function-in-tensorflow\n",
    "def loss_fn(input_img_tf, output_img_tf):\n",
    "    # -- get numpy of tf -- #\n",
    "    input_img = input_img_tf.numpy()\n",
    "    output_img = output_img_tf.numpy()\n",
    "    \n",
    "    #---- gradient loss ----#\n",
    "    grad_0_in, grad_45_in, grad_90_in, grad_135_in = tf.convert_to_tensor(ridge_gradient(input_img))\n",
    "    grad_0_out, grad_45_out, grad_90_out, grad_135_out = tf.convert_to_tensor(ridge_gradient(output_img))\n",
    "    \n",
    "\n",
    "    cost0 = tf.keras.losses.MSE(grad_0_out, grad_0_in)\n",
    "    cost45 = tf.keras.losses.MSE(grad_45_out, grad_45_in)\n",
    "    cost90 = tf.keras.losses.MSE(grad_90_out, grad_90_in)\n",
    "    cost135 = tf.keras.losses.MSE(grad_135_out, grad_135_in)\n",
    "#     cost0 = mse(grad_0_out, grad_0_in)\n",
    "#     cost45 = mse(grad_45_out, grad_45_in)\n",
    "#     cost90 = mse(grad_90_out, grad_90_in)\n",
    "#     cost135 = mse(grad_135_out, grad_135_in)\n",
    "    \n",
    "    grad_loss_sum = tf.math.add_n([cost0, cost45, cost90, cost135])\n",
    "    grad_loss = tf.math.divide(grad_loss_sum, 4)\n",
    "\n",
    "    # TODO OVER HERE >>> แก้ ride_orient\n",
    "    \n",
    "    #---- orientation loss and reliability loss ----#\n",
    "    ori_in, rel_in, _ = ridge_orientation(input_img)\n",
    "    ori_out, rel_out, _ = ridge_orientation(output_img)\n",
    "    \n",
    "    ori_in_tf = tf.convert_to_tensor(ridge_orientation(input_img))\n",
    "    rel_in_tf = tf.convert_to_tensor(ridge_orientation(input_img))\n",
    "    ori_out_tf = tf.convert_to_tensor(ridge_orientation(input_img))\n",
    "    rel_out_tf = tf.convert_to_tensor(ridge_orientation(output_img))\n",
    "    \n",
    "    ori_loss = tf.keras.losses.MSE(ori_out, ori_in)\n",
    "    rel_loss = tf.keras.losses.MSE(rel_out, rel_in)\n",
    "#     ori_loss = mse(ori_out, ori_in)\n",
    "#     rel_loss = mse(rel_out, rel_in)\n",
    "      \n",
    "    sum_loss = tf.math.add_n([grad_loss, ori_loss, rel_loss])\n",
    "    loss = tf.math.divide(sum_loss, 3)\n",
    "    \n",
    "    return loss\n",
    "#, grad_loss, ori_loss, rel_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-411ca902d26e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_loss_fn' is not defined"
     ]
    }
   ],
   "source": [
    "tf_loss_fn(train_x[0], train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters for Adam\n",
    "weight_decay = 0        \n",
    "momentum = 0.5          \n",
    "num_ch = 1     # num of channels in image\n",
    "lr = 0.0002       # initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_x, train_y, batch_size, epochs, exp_id):\n",
    "#     img = tf.Variable(img)\n",
    "# opt = tf.optimizers.Adam(learning_rate=lr, decay = 1e-6)\n",
    "\n",
    "# for _ in range(epoch):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         tape.watch(img)\n",
    "#         y = model(img.value())[:, :, :, filter]\n",
    "#         loss = -tf.math.reduce_mean(y)\n",
    "\n",
    "#     grads = tape.gradient(loss, img)\n",
    "#     opt.apply_gradients(zip([grads], [img]))\n",
    "    optimizers = Adam(learning_rate=lr, beta_1 = momentum, decay = weight_decay)\n",
    "\n",
    "    batch_round = math.ceil(len(train_x) / batch_size)\n",
    "    \n",
    "    loss = []\n",
    "    grad_loss = []\n",
    "    ori_loss = []\n",
    "    rel_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('epoch:', epoch, 'start...')\n",
    "        for batch in range(batch_round):\n",
    "            start_time = datetime.datetime.now()\n",
    "            \n",
    "            batch_start = batch * batch_size\n",
    "            batch_end = (batch_start + batch_size) - 1\n",
    "            \n",
    "            image_x_batched = tf.convert_to_tensor(train_x[batch_start: batch_end])\n",
    "            image_y_batched = tf.convert_to_tensor(train_y[batch_start: batch_end])\n",
    "            \n",
    "            for i in range(1):\n",
    "                print(i)\n",
    "                with tf.GradientTape() as tape:\n",
    "#                     print('HERE')\n",
    "                    tape.watch(model.trainable_variables)\n",
    "                    enhanced_image = model(tf.reshape(image_x_batched, (-1,96,96)))\n",
    "#                     print(enhanced_image.shape)\n",
    "#                     print(image_y_batched.shape)\n",
    "                \n",
    "                    total_loss = loss_fn(tf.reshape(image_y_batched, (image_y_batched.shape[0],image_y_batched.shape[1],image_y_batched.shape[2],1)), enhanced_image)\n",
    "\n",
    "#                     total_loss = tf.reduce_mean(enhanced_image)\n",
    "#                     print(total_loss)\n",
    "                  \n",
    "                gradients_of_model = tape.gradient(total_loss, model.trainable_variables)\n",
    "                \n",
    "                print('grad of model')\n",
    "                print(gradients_of_model)\n",
    "\n",
    "                optimizers.apply_gradients(zip(gradients_of_model, model.trainable_variables))\n",
    "                return 0, 0, 0, 0\n",
    "                \n",
    "            loss.append(total_loss)\n",
    "#             grad_loss.append(grad_l)\n",
    "#             ori_loss.append(ori_l)\n",
    "#             rel_loss.append(rel_l)\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "            print('loss:', total_loss)\n",
    "            print('batch:', batch, 'elapsed time:', elapsed_time)\n",
    "        \n",
    "    # SAVE MODELS\n",
    "    model.save(WEIGHT_SAVING_PATH + 'gae_'+ str(exp_id) +'.h5')\n",
    "            \n",
    "    return loss, grad_loss, ori_loss, rel_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 start...\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't convert non-rectangular Python sequence to Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-9245300f7186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-27542df99df5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_x, train_y, batch_size, epochs, exp_id)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#                     print(image_y_batched.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_y_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_y_batched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_y_batched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_y_batched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menhanced_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#                     total_loss = tf.reduce_mean(enhanced_image)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-e1fd366c4386>\u001b[0m in \u001b[0;36mloss_fn\u001b[0;34m(input_img_tf, output_img_tf)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#---- orientation loss and reliability loss ----#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mori_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mori_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge_orientation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1380\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                          as_ref=False):\n\u001b[1;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m    263\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 264\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
     ]
    }
   ],
   "source": [
    "loss, _,_,_ = train(model, train_x, train_y, 22, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_loss_fn(input_img, output_img):\n",
    "\n",
    "#     print('tf loss fn')\n",
    "#     print(type(tf.convert_to_tensor(input_img)), input_img.shape)\n",
    "#     print(type(output_img), output_img.shape)\n",
    "    \n",
    "    return tf.reduce_mean(input_img)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
